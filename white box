import numpy as np, 
import pandas as pd
from sklearn.preprocessing import LabelEncoder
class DenseLayer:
    def _init_(self, b):
        self.b = b
        
    def relu(self, inputs):
        return np.max(0, inputs)

    def softmax(self, inputs):
        exp_s = np.exp(inputs)
        probs = exp_s / np.sum(exp_s, axis=1, keepdims=True)
        return probs
    
    def relu_derivative(self, dA, Z):
        dZ = np.array(dA, copy = True)
        dZ[Z <= 0] = 0
        return dZ
    def forward(self, inputs, weights, bias, activation):
        Z_c = np.dot(inputs, weights.T) + bias
        
        if activation == “relu”:
            A_c = self.relu(inputs=Z_c)
        elif activation == “softmax”:
            A_c= self.softmax(inputs=Z_c)
            return A_c, Z_c
    def backward(self, dA_c, W_c, Z_c, A_p, activation):
        if activation == “softmax”:
            dW = np.dot(A_p.T, dA_c)
            db = np.sum(dA_c, axis=0, keepdims=True)
            dA = np.dot(dA_c, W_c) 
        else:
            dZ = self.relu_derivative(dA_c, Z_c)
            dW = np.dot(A_prev.T, Z)
            db = np.sum(dZ, axis=0, keepdims=True)
            dA = np.dot(dZ, W_curr)
         return dA, dW, db
class Network:
    def _init_(self):
        self.network = [] ## layers
        self.architecture = [] ## mapping input neurons --> output neurons
        self.params = [] ## W, b
        self.memory = [] ## Z, A
        self.gradients = [] ## dW, db
    def add(self, layer):
        self.network.append(layer)
    def _compile(self, data):
        for idx, layer in enumerate(self.network):
            if idx == 0:
                self.architecture.append({“input_dim”:data.shape[1], '”output_dim”:self.network[idx].neurons,
                                         “activation”:”relu”})
            elif idx > 0 and idx < len(self.network)-1:
                self.architecture.append({“input_dim”:self.network[idx-1].neurons, “output_dim”:self.network[idx].neurons,
                                         'activation':'relu'})
            else:
                self.architecture.append({“input_dim”:self.network[idx-1].neurons, “output_dim”:self.network[idx].neurons,
                                         “activation”:”softmax”})
        return self
    def _init_weights(self, data):
        self._compile(data)
         np.random.seed(99)
        for i in range(len(self.architecture)):
            self.params.append({
                'W':np.random.uniform(low=-1, high=1, 
                  size=(self.architecture[i]['output_dim'], 
                        self.architecture[i]['input_dim'])),
                'b':np.zeros((1, self.architecture[i]['output_dim']))})
        return self
     def _forwardprop(self, data):
        A_curr = data
        for i in range(len(self.params)):
            A_p = A_c
            A_c, Z_c = self.network[i].forward(inputs=A_p, weights=self.params[i]['W'], 
                                           bias=self.params[i]['b'], activation=self.architecture[i]['activation'])
         self.memory.append({'inputs':A_p, 'Z':Z_c})
         return A_c
     def _backprop(self, predicted, actual):
        num_samples = len(actual)
        dscores = predicted
        dscores[range(num_samples),actual] -= 1
        dscores /= num_samples
        dA_p = dscores
        for idx, layer in reversed(list(enumerate(self.network))):
            dA_c = dA_p
            A_p = self.memory[idx]['inputs']
            Z_c = self.memory[idx]['Z']
            W_c = self.params[idx]['W']
            activation = self.architecture[idx]['activation']
            dA_p, dW_c, db_c = layer.backward(dA_curr, W_curr, Z_curr, A_prev, activation)
            self.gradients.append({'dW':dW_c, 'db':db_c})
    def _update(self, lr=0.01):
        for idx, layer in enumerate(self.network):
            self.params[idx]['W'] -= lr * list(reversed(self.gradients))[idx]['dW'].T  
            self.params[idx]['b'] -= lr * list(reversed(self.gradients))[idx]['db']
    def _get_accuracy(self, predicted, actual):
        return np.mean(np.argmax(predicted, axis=1)==actual)
    def _calculate_loss(self, predicted, actual):
        samples = len(actual)
        correct_logprobs = -np.log(predicted[range(samples),actual])
        data_loss = np.sum(correct_logprobs)/samples
        return data_loss
    def train(self, X_train, y_train, epochs):
        self.loss = []
        self.accuracy = []
        self._init_weights(X_train)
        for i in range(epochs):
            yhat = self._forwardprop(X_train)
            self.accuracy.append(self._get_accuracy(predicted=yhat, actual=y_train))
            self.loss.append(self._calculate_loss(predicted=yhat, actual=y_train))
            self._backprop(predicted=yhat, actual=y_train)
            self._update()
            if i % 20 == 0:
                s = 'EPOCH: {}, ACCURACY: {}, LOSS: {}'.format(i, self.accuracy[-1], self.loss[-1])
                print(s)
if _name_ == '_main_':
    def get_data(path):
        data = pd.read_csv(path, index_col=0)
        cols = list(data.columns)
        target = cols.pop() 
        X = data[cols].copy()
        y = data[target].copy()
        y = LabelEncoder().fit_transform(y)
        return np.array(X), np.array(y)
   X, y = get_data(r'C:\Users\12482\Desktop\articles\nn_from_scratch\iris.csv')
   model = Network()
   model.add(DenseLayer(6))
   model.add(DenseLayer(8))
   model.add(DenseLayer(10))
   model.add(DenseLayer(3))
   model.train(X_train=X, y_train=y, epochs=200)
